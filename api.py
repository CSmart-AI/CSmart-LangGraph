"""
get_answer() í•¨ìˆ˜ í•˜ë‚˜ë¡œ ëª¨ë“  ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    from api import get_answer
    
    result = get_answer("ì¤‘ì•™ëŒ€í•™êµ ì´ê³¼ í¸ì…ì€ ì–´ë–¤ ê³¼ëª©ì„ ì¤€ë¹„í•´ì•¼ í•˜ë‚˜ìš”?")
    print(result["final_answer"])
"""

from typing import Dict, List, Optional
from dotenv import load_dotenv
import os
import requests
from pydantic import BaseModel
from typing import Literal

# ======================================
# 1ë‹¨ê³„: í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# ======================================
load_dotenv()

# ======================================
# 2ë‹¨ê³„: ëª¨ë“  í•„ìš”í•œ ëª¨ë“ˆ import ë° ì´ˆê¸°í™”
# ======================================
print("CSmart API ì´ˆê¸°í™” ì¤‘...")

# States
from step2_states import QAState, prepare_context

# DB and Search (ìë™ìœ¼ë¡œ ì´ˆê¸°í™”ë¨)
from step3_db_and_search import guideline_search, web_search, tools

# LLM (ìë™ìœ¼ë¡œ ì´ˆê¸°í™”ë¨)
from step4_llm import llm, llm_with_tools

# Agents (ìë™ìœ¼ë¡œ ì»´íŒŒì¼ë¨)
from step5_guideline_agent import guideline_agent
from step6_web_agent import search_web_agent
from step7_integrated_agent import integrated_agent

print("CSmart API ì´ˆê¸°í™” ì™„ë£Œ!\n")


# ======================================
# ğŸ¯ ì§ˆë¬¸ ë³µì¡ë„ íŒë³„ì„ ìœ„í•œ ë°ì´í„° ëª¨ë¸
# ======================================
class QuestionComplexity(BaseModel):
    """ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ íŒë³„í•©ë‹ˆë‹¤."""
    complexity: Literal["simple", "complex"] = None
    reason: str = None


# ======================================
# ğŸ¯ íŒŒì¸íŠœë‹ ëª¨ë¸ ë‹µë³€ í’ˆì§ˆ í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„° ëª¨ë¸
# ======================================
class AnswerQuality(BaseModel):
    """íŒŒì¸íŠœë‹ ëª¨ë¸ ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤."""
    quality: Literal["good", "poor"] = None
    reason: str = None
    score: int = None  # 1-10 ì ìˆ˜


# ======================================
# ğŸ¤– ê°„ë‹¨í•œ ì§ˆë¬¸ íŒë³„ í•¨ìˆ˜
# ======================================
def is_simple_question(question: str, verbose: bool = True) -> bool:
    """
    ì§ˆë¬¸ì´ ê°„ë‹¨í•œ ì¼ë°˜ì ì¸ í•™ìŠµ ì¡°ì–¸ì¸ì§€, 
    íŠ¹ì • ëŒ€í•™/ì¼ì •/ì „í˜• ì •ë³´ê°€ í•„ìš”í•œ ë³µì¡í•œ ì§ˆë¬¸ì¸ì§€ íŒë³„í•©ë‹ˆë‹¤.
    
    ê°„ë‹¨í•œ ì§ˆë¬¸ ì˜ˆì‹œ:
    - "ìˆ˜í•™ ê³µë¶€ëŠ” ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?"
    - "ì˜¤ë‹µë…¸íŠ¸ëŠ” ì–´ë–»ê²Œ ì •ë¦¬í• ê¹Œìš”?"
    - "ì˜ì–´ ë‹¨ì–´ ì•”ê¸°ëŠ” ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?"
    
    ë³µì¡í•œ ì§ˆë¬¸ ì˜ˆì‹œ:
    - "ì¤‘ì•™ëŒ€í•™êµ ì´ê³¼ í¸ì…ì€ ì–´ë–¤ ê³¼ëª©ì„ ì¤€ë¹„í•´ì•¼ í•˜ë‚˜ìš”?" (íŠ¹ì • ëŒ€í•™/ê³„ì—´)
    - "2025í•™ë…„ë„ í¸ì… ì‹œí—˜ ì¼ì •ì€ ì–¸ì œì¸ê°€ìš”?" (êµ¬ì²´ì ì¸ ë‚ ì§œ ì •ë³´)
    """
    from langchain_core.prompts import ChatPromptTemplate
    
    try:
        # êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ìœ„í•œ LLM ì„¤ì •
        structured_llm = llm.with_structured_output(QuestionComplexity)
        
        # íŒë³„ í”„ë¡¬í”„íŠ¸
        system_prompt = """ë‹¹ì‹ ì€ ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ íŒë³„í•˜ëŠ” ë¶„ë¥˜ê¸°ì…ë‹ˆë‹¤.

ì§ˆë¬¸ì„ ë‹¤ìŒ ë‘ ê°€ì§€ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:

1. **simple (ê°„ë‹¨í•œ ì§ˆë¬¸)**:
   - ì¼ë°˜ì ì¸ í•™ìŠµ ë°©ë²•, ê³µë¶€ ì¡°ì–¸, í•™ìŠµ ì „ëµì— ëŒ€í•œ ì§ˆë¬¸
   - íŠ¹ì • ëŒ€í•™ëª…, ì—°ë„, ì¼ì •ì´ í¬í•¨ë˜ì§€ ì•Šì€ ì¼ë°˜ì ì¸ ì§ˆë¬¸
   - ì˜ˆ: "ìˆ˜í•™ ê³µë¶€ëŠ” ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?", "ì˜¤ë‹µë…¸íŠ¸ ì •ë¦¬ë²•", "ì˜ì–´ ë‹¨ì–´ ì•”ê¸°ë²•"

2. **complex (ë³µì¡í•œ ì§ˆë¬¸)**:
   - íŠ¹ì • ëŒ€í•™ëª…ì´ í¬í•¨ëœ ì§ˆë¬¸ (ì˜ˆ: ì¤‘ì•™ëŒ€, ì—°ì„¸ëŒ€, ê³ ë ¤ëŒ€ ë“±)
   - íŠ¹ì • ì—°ë„ë‚˜ ì¼ì •ì„ ë¬¼ì–´ë³´ëŠ” ì§ˆë¬¸ (ì˜ˆ: 2025í•™ë…„ë„, ì‹œí—˜ ì¼ì •)
   - êµ¬ì²´ì ì¸ ì „í˜•/ëª¨ì§‘ìš”ê°•/ì‹œí—˜ ê³¼ëª© ë“±ì„ ë¬¼ì–´ë³´ëŠ” ì§ˆë¬¸
   - ê²€ìƒ‰ì´ë‚˜ ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒê°€ í•„ìš”í•œ ì§ˆë¬¸

íŒë‹¨ ê·¼ê±°ë¥¼ reason í•„ë“œì— ê°„ë‹¨íˆ ì ì–´ì£¼ì„¸ìš”."""

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ë¥˜í•˜ì„¸ìš”:\n\n{question}")
        ])
        
        # íŒë³„ ì‹¤í–‰
        chain = prompt | structured_llm
        result = chain.invoke({"question": question})
        
        if verbose:
            print(f"\nì§ˆë¬¸ ë³µì¡ë„ íŒë³„:")
            print(f"   - ì§ˆë¬¸: {question}")
            print(f"   - íŒë‹¨: {result.complexity}")
            print(f"   - ì´ìœ : {result.reason}\n")
        
        return result.complexity == "simple"
        
    except Exception as e:
        if verbose:
            print(f"ì§ˆë¬¸ ë³µì¡ë„ íŒë³„ ì˜¤ë¥˜ (ê¸°ë³¸ê°’: complex): {str(e)[:100]}")
        # ì˜¤ë¥˜ ì‹œ ì•ˆì „í•˜ê²Œ ë³µì¡í•œ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬ (ê¸°ì¡´ LangGraph ì‚¬ìš©)
        return False


# ======================================
# ğŸ¯ íŒŒì¸íŠœë‹ ëª¨ë¸ ë‹µë³€ ì¬ê°€ê³µ í•¨ìˆ˜
# ======================================
def refine_finetuned_answer(question: str, raw_answer: str, verbose: bool = True) -> str:
    """
    íŒŒì¸íŠœë‹ ëª¨ë¸ì˜ ì›ì‹œ ë‹µë³€ì„ LLMìœ¼ë¡œ ì¬ê°€ê³µí•˜ì—¬ ë” ì™„ì„±ë„ ë†’ì€ ë‹µë³€ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
    
    Parameters:
    -----------
    question : str
        ì›ë³¸ ì§ˆë¬¸
    raw_answer : str
        íŒŒì¸íŠœë‹ ëª¨ë¸ì´ ìƒì„±í•œ ì›ì‹œ ë‹µë³€
    verbose : bool
        ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
    --------
    str
        ì¬ê°€ê³µëœ ë‹µë³€
    """
    from langchain_core.prompts import ChatPromptTemplate
    
    try:
        # ì¬ê°€ê³µ í”„ë¡¬í”„íŠ¸
        system_prompt = """ë‹¹ì‹ ì€ í¸ì… ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. íŒŒì¸íŠœë‹ ëª¨ë¸ì´ ìƒì„±í•œ ë‹µë³€ì„ ë°›ì•„ì„œ ê°„ê²°í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ìœ¼ë¡œ ì¬ê°€ê³µí•´ì£¼ì„¸ìš”.

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€ì„ ê°œì„ í•˜ì„¸ìš”:

**ê°œì„  ë°©í–¥:**
1. **ê°„ê²°ì„±**: í•µì‹¬ ë‚´ìš©ë§Œ 1-3ì¤„ë¡œ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì •ë¦¬
2. **êµ¬ì²´ì„±**: ëª¨í˜¸í•œ í‘œí˜„ì„ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸ìœ¼ë¡œ ë³€ê²½
3. **ì‹¤ìš©ì„±**: í•™ìƒì´ ì‹¤ì œë¡œ ë”°ë¼í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ë°©ë²• ì œì‹œ
4. **ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„**: ë¶€ìì—°ìŠ¤ëŸ¬ìš´ ë¶€ë¶„ì„ ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰½ê²Œ ìˆ˜ì •

**ì£¼ì˜ì‚¬í•­:**
- ë‹µë³€ì€ ë°˜ë“œì‹œ 1-3ì¤„ ì´ë‚´ë¡œ ì‘ì„±
- í•µì‹¬ ë‚´ìš©ë§Œ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì „ë‹¬
- í¸ì… ìƒë‹´ì— ì í•©í•œ ì „ë¬¸ì ì¸ í†¤ ìœ ì§€
- êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸ ì œê³µ
- íŠ¹ìˆ˜ë¬¸ìë‚˜ ë¶ˆí•„ìš”í•œ í˜•ì‹ ì œê±°

ì›ë³¸ ì§ˆë¬¸ê³¼ íŒŒì¸íŠœë‹ ëª¨ë¸ì˜ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•œ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”."""

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "ë‹¤ìŒ ì§ˆë¬¸ê³¼ íŒŒì¸íŠœë‹ ëª¨ë¸ì˜ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ê°œì„ ëœ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:\n\n[ì§ˆë¬¸]\n{question}\n\n[íŒŒì¸íŠœë‹ ëª¨ë¸ ë‹µë³€]\n{raw_answer}\n\n[ê°œì„ ëœ ë‹µë³€]")
        ])
        
        # ì¬ê°€ê³µ ì‹¤í–‰
        chain = prompt | llm
        refined_answer = chain.invoke({"question": question, "raw_answer": raw_answer})
        
        # íŠ¹ìˆ˜ë¬¸ì ë° ë¶ˆí•„ìš”í•œ í˜•ì‹ ì œê±°
        if hasattr(refined_answer, 'content'):
            refined_answer = refined_answer.content
        elif isinstance(refined_answer, str):
            # content= ê°™ì€ íŠ¹ìˆ˜ë¬¸ì ì œê±°
            refined_answer = refined_answer.replace('content=', '').strip()
            # ë”°ì˜´í‘œ ì œê±°
            if refined_answer.startswith("'") and refined_answer.endswith("'"):
                refined_answer = refined_answer[1:-1]
            elif refined_answer.startswith('"') and refined_answer.endswith('"'):
                refined_answer = refined_answer[1:-1]
        
        if verbose:
            print(f"\níŒŒì¸íŠœë‹ ë‹µë³€ ì¬ê°€ê³µ:")
            print(f"   - ì›ë³¸ ì§ˆë¬¸: {question}")
            print(f"   - ì›ì‹œ ë‹µë³€: {raw_answer[:100]}{'...' if len(raw_answer) > 100 else ''}")
            print(f"   - ì¬ê°€ê³µ ì™„ë£Œ: {len(refined_answer)}ì\n")
        
        return refined_answer
        
    except Exception as e:
        if verbose:
            print(f"ë‹µë³€ ì¬ê°€ê³µ ì˜¤ë¥˜ (ì›ë³¸ ë‹µë³€ ì‚¬ìš©): {str(e)[:100]}")
        # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ë‹µë³€ ë°˜í™˜
        return raw_answer


# ======================================
# ğŸ¯ íŒŒì¸íŠœë‹ ëª¨ë¸ ë‹µë³€ í’ˆì§ˆ í‰ê°€ í•¨ìˆ˜
# ======================================
def evaluate_answer_quality(question: str, answer: str, verbose: bool = True) -> bool:
    """
    íŒŒì¸íŠœë‹ ëª¨ë¸ì˜ ë‹µë³€ì´ ì§ˆë¬¸ì— ì ì ˆíˆ ë‹µë³€í–ˆëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    question : str
        ì›ë³¸ ì§ˆë¬¸
    answer : str
        íŒŒì¸íŠœë‹ ëª¨ë¸ì´ ìƒì„±í•œ ë‹µë³€
    verbose : bool
        ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
    --------
    bool
        True: ë‹µë³€ì´ ì¶©ë¶„íˆ ì¢‹ìŒ (í’ˆì§ˆ ê¸°ì¤€ í†µê³¼)
        False: ë‹µë³€ì´ ë¶€ì¡±í•¨ (LangGraphë¡œ ì¬ì‹œë„ í•„ìš”)
    """
    from langchain_core.prompts import ChatPromptTemplate
    
    try:
        # êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ìœ„í•œ LLM ì„¤ì •
        structured_llm = llm.with_structured_output(AnswerQuality)
        
        # í‰ê°€ í”„ë¡¬í”„íŠ¸
        system_prompt = """ë‹¹ì‹ ì€ í¸ì… ìƒë‹´ ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€ì„ í‰ê°€í•˜ì„¸ìš”:

**ì¢‹ì€ ë‹µë³€ (good) ê¸°ì¤€:**
- ì§ˆë¬¸ì— ì§ì ‘ì ì´ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•¨
- ì‹¤ìš©ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸ì„ ì œê³µí•¨
- í¸ì… ìƒë‹´ì— ì í•©í•œ ì „ë¬¸ì ì¸ ë‚´ìš©ì„
- ë‹µë³€ì´ ì¶©ë¶„íˆ ìƒì„¸í•˜ê³  ë„ì›€ì´ ë¨
- ì˜¤ë¥˜ë‚˜ ë¶€ì •í™•í•œ ì •ë³´ê°€ ì—†ìŒ

**ë¶€ì¡±í•œ ë‹µë³€ (poor) ê¸°ì¤€:**
- ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ë¶ˆì™„ì „í•¨
- "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤", "í™•ì¸í•´ë³´ì„¸ìš”" ë“±ìœ¼ë¡œ ëë‚¨
- ë„ˆë¬´ ì§§ê±°ë‚˜ ì¼ë°˜ì ì¸ ë‚´ìš©ë§Œ í¬í•¨
- ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì„
- ì˜¤ë¥˜ë‚˜ ë¶€ì •í™•í•œ ì •ë³´ê°€ í¬í•¨ë¨

ì ìˆ˜ ê¸°ì¤€:
- 8-10ì : ë§¤ìš° ì¢‹ì€ ë‹µë³€ (good)
- 6-7ì : ë³´í†µ ë‹µë³€ (good)
- 4-5ì : ë¶€ì¡±í•œ ë‹µë³€ (poor)
- 1-3ì : ë§¤ìš° ë¶€ì¡±í•œ ë‹µë³€ (poor)

quality í•„ë“œì— "good" ë˜ëŠ” "poor"ë¥¼, score í•„ë“œì— 1-10 ì ìˆ˜ë¥¼, reason í•„ë“œì— í‰ê°€ ê·¼ê±°ë¥¼ ì ì–´ì£¼ì„¸ìš”."""

        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "ë‹¤ìŒ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í‰ê°€í•˜ì„¸ìš”:\n\n[ì§ˆë¬¸]\n{question}\n\n[ë‹µë³€]\n{answer}")
        ])
        
        # í‰ê°€ ì‹¤í–‰
        chain = prompt | structured_llm
        result = chain.invoke({"question": question, "answer": answer})
        
        if verbose:
            print(f"\në‹µë³€ í’ˆì§ˆ í‰ê°€:")
            print(f"   - ì§ˆë¬¸: {question}")
            print(f"   - ë‹µë³€: {answer[:100]}{'...' if len(answer) > 100 else ''}")
            print(f"   - í’ˆì§ˆ: {result.quality}")
            print(f"   - ì ìˆ˜: {result.score}/10")
            print(f"   - ì´ìœ : {result.reason}\n")
        
        # 6ì  ì´ìƒì´ë©´ ì¢‹ì€ ë‹µë³€ìœ¼ë¡œ íŒë‹¨
        return result.score >= 6
        
    except Exception as e:
        if verbose:
            print(f"ë‹µë³€ í’ˆì§ˆ í‰ê°€ ì˜¤ë¥˜ (ê¸°ë³¸ê°’: poor): {str(e)[:100]}")
        # ì˜¤ë¥˜ ì‹œ ì•ˆì „í•˜ê²Œ ë¶€ì¡±í•œ ë‹µë³€ìœ¼ë¡œ ì²˜ë¦¬ (LangGraph ì‚¬ìš©)
        return False


# ======================================
# ğŸ“ íŒŒì¸íŠœë‹ ëª¨ë¸ API í˜¸ì¶œ í•¨ìˆ˜
# ======================================
def call_finetuned_model(
    question: str,
    max_tokens: int = 100,
    temperature: float = 0.3,
    top_k: int = 50,
    top_p: float = 0.95,
    repetition_penalty: float = 1.2,
    timeout: int = 120,
    max_retries: int = 3
) -> str:
    """
    CSmart-FAQ íŒŒì¸íŠœë‹ ëª¨ë¸ APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Parameters:
    -----------
    question : str
        ì§ˆë¬¸ ë‚´ìš©
    max_tokens : int
        ìƒì„±í•  ë‹µë³€ì˜ ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’: 100)
    temperature : float
        ë‹µë³€ì˜ ë‹¤ì–‘ì„± (0.1~1.0, ê¸°ë³¸ê°’: 0.3)
    top_k : int
        Top-K ìƒ˜í”Œë§ (ê¸°ë³¸ê°’: 50)
    top_p : float
        Top-P (nucleus) ìƒ˜í”Œë§ (ê¸°ë³¸ê°’: 0.95)
    repetition_penalty : float
        ë°˜ë³µ í˜ë„í‹° (ê¸°ë³¸ê°’: 1.2)
    timeout : int
        íƒ€ì„ì•„ì›ƒ (ì´ˆ, ê¸°ë³¸ê°’: 120)
    max_retries : int
        ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 3)
    
    Returns:
    --------
    str
        ìƒì„±ëœ ë‹µë³€ ë˜ëŠ” ì˜¤ë¥˜ ë©”ì‹œì§€
    """
    url = "https://csmart-ai-faq-finetuning.hf.space/predict"
    
    payload = {
        "question": question,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_k": top_k,
        "top_p": top_p,
        "repetition_penalty": repetition_penalty
    }
    
    for attempt in range(max_retries):
        try:
            print(f"íŒŒì¸íŠœë‹ ëª¨ë¸ í˜¸ì¶œ ì¤‘... (ì‹œë„ {attempt + 1}/{max_retries})")
            
            response = requests.post(url, json=payload, timeout=timeout)
            
            if response.status_code == 200:
                result = response.json()
                answer = result.get("answer", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("íŒŒì¸íŠœë‹ ëª¨ë¸ ë‹µë³€ ìƒì„± ì™„ë£Œ")
                return answer
                
            elif response.status_code == 400:
                error_msg = "ì˜ëª»ëœ ìš”ì²­ì…ë‹ˆë‹¤. íŒŒë¼ë¯¸í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
                print(f"{error_msg}")
                return f"ì˜¤ë¥˜: {error_msg}"
                
            elif response.status_code == 500:
                print(f"ì„œë²„ ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    continue  # ì¬ì‹œë„
                else:
                    return "ì˜¤ë¥˜: ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            else:
                print(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {response.status_code}")
                return f"ì˜¤ë¥˜: ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (ìƒíƒœ ì½”ë“œ: {response.status_code})"
                
        except requests.exceptions.Timeout:
            print(f"ìš”ì²­ ì‹œê°„ ì´ˆê³¼ (ì‹œë„ {attempt + 1}/{max_retries})")
            if attempt < max_retries - 1:
                continue  # ì¬ì‹œë„
            else:
                return "ì˜¤ë¥˜: ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤."
                
        except requests.exceptions.RequestException as e:
            print(f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {str(e)[:100]}")
            if attempt < max_retries - 1:
                continue  # ì¬ì‹œë„
            else:
                return f"ì˜¤ë¥˜: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ({str(e)[:50]})"
    
    return "ì˜¤ë¥˜: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤."


# ======================================
# ğŸ¯ ë©”ì¸ API í•¨ìˆ˜ (ğŸ†• ë¼ìš°íŒ… ë¡œì§ í¬í•¨)
# ======================================
def get_answer(
    question: str,
    student_profile: Optional[Dict[str, str]] = None,
    recent_dialogues: Optional[List[Dict[str, str]]] = None,
    verbose: bool = True,
    force_mode: Optional[Literal["simple", "complex"]] = None
) -> Dict:
    """
    í¸ì… ìƒë‹´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    ì§ˆë¬¸ì˜ ë³µì¡ë„ì— ë”°ë¼ ìë™ìœ¼ë¡œ ë¼ìš°íŒ…ë©ë‹ˆë‹¤:
    - ê°„ë‹¨í•œ ì§ˆë¬¸ (ì¼ë°˜ì ì¸ í•™ìŠµ ì¡°ì–¸) â†’ íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš© â†’ LLM ì¬ê°€ê³µ â†’ ë‹µë³€ í’ˆì§ˆ í‰ê°€ â†’ ê¸°ì¤€ ë¯¸ë‹¬ ì‹œ LangGraph ì¬ì‹œë„
    - ë³µì¡í•œ ì§ˆë¬¸ (íŠ¹ì • ëŒ€í•™/ì¼ì •/ì „í˜• ì •ë³´) â†’ LangGraph ì—ì´ì „íŠ¸ ì‚¬ìš©
    
    Parameters:
    -----------
    question : str
        í•™ìƒì˜ ì§ˆë¬¸ (ì˜ˆ: "ì¤‘ì•™ëŒ€í•™êµ ì´ê³¼ í¸ì…ì€ ì–´ë–¤ ê³¼ëª©ì„ ì¤€ë¹„í•´ì•¼ í•˜ë‚˜ìš”?")
    
    student_profile : dict, optional
        í•™ìƒ í”„ë¡œí•„ ì •ë³´
        ì˜ˆ: {"target_university": "ì¤‘ì•™ëŒ€í•™êµ", "track": "ì´ê³¼"}
        ê¸°ë³¸ê°’: {"target_university": "ë¯¸ì§€ì •", "track": "ê³„ì—´ ë¯¸ì§€ì •"}
    
    recent_dialogues : list, optional
        ìµœê·¼ ëŒ€í™” ë‚´ì—­ (ìµœëŒ€ 5ê°œ)
        ì˜ˆ: [
            {"role": "student", "message": "..."},
            {"role": "teacher", "message": "..."}
        ]
        ê¸°ë³¸ê°’: ë¹ˆ ë¦¬ìŠ¤íŠ¸
    
    verbose : bool, optional
        ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        Falseë¡œ ì„¤ì •í•˜ë©´ ë¡œê·¸ë¥¼ ìˆ¨ê¹ë‹ˆë‹¤.
    
    force_mode : Literal["simple", "complex"], optional
        ìˆ˜ë™ìœ¼ë¡œ íŠ¹ì • ëª¨ë“œë¥¼ ì„ íƒí•˜ë„ë¡ ì§€ì • (ê¸°ë³¸ê°’: None, ìë™ íŒë³„)
        - "simple": íŒŒì¸íŠœë‹ ëª¨ë¸ ìˆ˜ë™ ì„ íƒ
        - "complex": LangGraph ì—ì´ì „íŠ¸ ìˆ˜ë™ ì„ íƒ
    
    Returns:
    --------
    dict
        {
            "question": str,           # ì›ë³¸ ì§ˆë¬¸
            "final_answer": str,       # ìµœì¢… ë‹µë³€
            "model_used": str,         # ì‚¬ìš©ëœ ëª¨ë¸ ("finetuned_refined", "langgraph", "langgraph_fallback")
            "context": str,            # ìƒì„±ëœ ì»¨í…ìŠ¤íŠ¸ (LangGraph ì‚¬ìš© ì‹œ)
            "datasources": list,       # ì‚¬ìš©ëœ ë°ì´í„° ì†ŒìŠ¤ (LangGraph ì‚¬ìš© ì‹œ)
            "success": bool,           # ì„±ê³µ ì—¬ë¶€
            "error": str or None       # ì˜¤ë¥˜ ë©”ì‹œì§€ (ìˆëŠ” ê²½ìš°)
        }
    
    Examples:
    ---------
    >>> from api import get_answer
    >>> 
    >>> # ê°„ë‹¨í•œ ì§ˆë¬¸ (ìë™ìœ¼ë¡œ íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš©)
    >>> result = get_answer("ìˆ˜í•™ ê³µë¶€ëŠ” ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?")
    >>> print(f"ì‚¬ìš©ëœ ëª¨ë¸: {result['model_used']}")  # "finetuned_refined"
    >>> print(result["final_answer"])
    >>> 
    >>> # ë³µì¡í•œ ì§ˆë¬¸ (ìë™ìœ¼ë¡œ LangGraph ì‚¬ìš©)
    >>> result = get_answer("ì¤‘ì•™ëŒ€í•™êµ ì´ê³¼ í¸ì…ì€ ì–´ë–¤ ê³¼ëª©ì„ ì¤€ë¹„í•´ì•¼ í•˜ë‚˜ìš”?")
    >>> print(f"ì‚¬ìš©ëœ ëª¨ë¸: {result['model_used']}")  # "langgraph"
    >>> print(result["final_answer"])
    >>> 
    >>> # í”„ë¡œí•„ê³¼ ëŒ€í™” ë‚´ì—­ í¬í•¨
    >>> result = get_answer(
    ...     question="ì¤‘ì•™ëŒ€í•™êµ ì´ê³¼ í¸ì…ì€ ì–´ë–¤ ê³¼ëª©ì„ ì¤€ë¹„í•´ì•¼ í•˜ë‚˜ìš”?",
    ...     student_profile={"target_university": "ì¤‘ì•™ëŒ€í•™êµ", "track": "ì´ê³¼"},
    ...     recent_dialogues=[
    ...         {"role": "student", "message": "í¸ì… ì¤€ë¹„ë¥¼ ì‹œì‘í•˜ë ¤ê³  í•©ë‹ˆë‹¤."},
    ...         {"role": "teacher", "message": "ì–´ëŠ ëŒ€í•™ì„ ëª©í‘œë¡œ í•˜ì‹œë‚˜ìš”?"}
    ...     ]
    ... )
    >>> print(result["final_answer"])
    >>> 
    >>> # ë¡œê·¸ ìˆ¨ê¸°ê¸°
    >>> result = get_answer("ì§ˆë¬¸", verbose=False)
    >>> print(result["final_answer"])
    >>> 
    >>> # ìˆ˜ë™ìœ¼ë¡œ íŠ¹ì • ëª¨ë“œ ì„ íƒ
    >>> result = get_answer("ìˆ˜í•™ ê³µë¶€ë²•", force_mode="complex")  # ìˆ˜ë™ìœ¼ë¡œ LangGraph ì„ íƒ
    """
    
    # ê¸°ë³¸ê°’ ì„¤ì •
    if student_profile is None:
        student_profile = {
            "target_university": "ë¯¸ì§€ì •",
            "track": "ê³„ì—´ ë¯¸ì§€ì •"
        }
    
    if recent_dialogues is None:
        recent_dialogues = []
    
    try:
        # ë¡œê·¸ ì¶œë ¥ ì œì–´
        if not verbose:
            import sys
            from io import StringIO
            old_stdout = sys.stdout
            sys.stdout = StringIO()
        
        # ==========================================
        # ğŸ”€ 1ë‹¨ê³„: ì§ˆë¬¸ ë³µì¡ë„ íŒë³„ ë° ë¼ìš°íŒ…
        # ==========================================
        if force_mode:
            # ìˆ˜ë™ ì„ íƒ ëª¨ë“œê°€ ì§€ì •ëœ ê²½ìš°
            use_simple_model = (force_mode == "simple")
            if verbose or force_mode:
                print(f"\nìˆ˜ë™ ì„ íƒ ëª¨ë“œ: {force_mode}")
        else:
            # ìë™ íŒë³„
            use_simple_model = is_simple_question(question, verbose=verbose)
        
        # ==========================================
        # ğŸ“ 2ë‹¨ê³„: ê°„ë‹¨í•œ ì§ˆë¬¸ â†’ íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš©
        # ==========================================
        if use_simple_model:
            print("\n" + "="*60)
            print("ë¼ìš°íŒ… ê²°ì •: íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš© (ê°„ë‹¨í•œ ì§ˆë¬¸)")
            print("="*60)
            
            # íŒŒì¸íŠœë‹ ëª¨ë¸ í˜¸ì¶œ
            answer = call_finetuned_model(
                question=question,
                max_tokens=100,
                temperature=0.3,
                timeout=120,
                max_retries=3
            )
            
            # íŒŒì¸íŠœë‹ ëª¨ë¸ ë‹µë³€ ì¬ê°€ê³µ ë° í’ˆì§ˆ í‰ê°€
            if not answer.startswith("ì˜¤ë¥˜:"):
                # 1ë‹¨ê³„: íŒŒì¸íŠœë‹ ë‹µë³€ì„ LLMìœ¼ë¡œ ì¬ê°€ê³µ
                refined_answer = refine_finetuned_answer(question, answer, verbose=verbose)
                
                # 2ë‹¨ê³„: ì¬ê°€ê³µëœ ë‹µë³€ì˜ í’ˆì§ˆ í‰ê°€
                is_good_answer = evaluate_answer_quality(question, refined_answer, verbose=verbose)
                
                if is_good_answer:
                    # í’ˆì§ˆì´ ì¢‹ìœ¼ë©´ ì¬ê°€ê³µëœ ë‹µë³€ ì‚¬ìš©
                    print("íŒŒì¸íŠœë‹ ëª¨ë¸ ë‹µë³€ ì¬ê°€ê³µ ë° í’ˆì§ˆ í†µê³¼ - ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì‚¬ìš©")
                    
                    if not verbose:
                        sys.stdout = old_stdout
                    
                    return {
                        "question": question,
                        "final_answer": refined_answer,
                        "model_used": "finetuned_refined",
                        "context": "",
                        "datasources": ["finetuned_model", "llm_refinement"],
                        "success": True,
                        "error": None
                    }
                else:
                    # í’ˆì§ˆì´ ë¶€ì¡±í•˜ë©´ LangGraphë¡œ ì¬ì‹œë„
                    print("íŒŒì¸íŠœë‹ ëª¨ë¸ ë‹µë³€ í’ˆì§ˆ ë¯¸ë‹¬ - LangGraphë¡œ ì¬ì‹œë„")
                    print("\n" + "="*60)
                    print("ì¬ë¼ìš°íŒ…: LangGraph ì—ì´ì „íŠ¸ ì‚¬ìš© (ë‹µë³€ í’ˆì§ˆ ë¯¸ë‹¬)")
                    print("="*60)
                    
                    # ì…ë ¥ ë°ì´í„° êµ¬ì„±
                    inputs = {
                        "question": question,
                        "student_profile": student_profile,
                        "recent_dialogues": recent_dialogues
                    }
                    
                    # í†µí•© ì—ì´ì „íŠ¸ ì‹¤í–‰
                    result = integrated_agent.invoke(
                        inputs,
                        config={
                            "recursion_limit": 25,  # ì¬ê·€ ì œí•œ
                            "timeout": 120          # 2ë¶„ íƒ€ì„ì•„ì›ƒ
                        }
                    )
                    
                    if not verbose:
                        sys.stdout = old_stdout
                    
                    # ê²°ê³¼ ë°˜í™˜
                    return {
                        "question": question,
                        "final_answer": result.get("final_answer", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."),
                        "model_used": "langgraph_fallback",
                        "context": result.get("context", ""),
                        "datasources": result.get("datasources", []),
                        "success": True,
                        "error": None
                    }
            else:
                # íŒŒì¸íŠœë‹ ëª¨ë¸ ì˜¤ë¥˜ ì‹œ LangGraphë¡œ ì¬ì‹œë„
                print("íŒŒì¸íŠœë‹ ëª¨ë¸ ì˜¤ë¥˜ - LangGraphë¡œ ì¬ì‹œë„")
                print("\n" + "="*60)
                print("ì¬ë¼ìš°íŒ…: LangGraph ì—ì´ì „íŠ¸ ì‚¬ìš© (íŒŒì¸íŠœë‹ ëª¨ë¸ ì˜¤ë¥˜)")
                print("="*60)
                
                # ì…ë ¥ ë°ì´í„° êµ¬ì„±
                inputs = {
                    "question": question,
                    "student_profile": student_profile,
                    "recent_dialogues": recent_dialogues
                }
                
                # í†µí•© ì—ì´ì „íŠ¸ ì‹¤í–‰
                result = integrated_agent.invoke(
                    inputs,
                    config={
                        "recursion_limit": 25,  # ì¬ê·€ ì œí•œ
                        "timeout": 120          # 2ë¶„ íƒ€ì„ì•„ì›ƒ
                    }
                )
                
                if not verbose:
                    sys.stdout = old_stdout
                
                # ê²°ê³¼ ë°˜í™˜
                return {
                    "question": question,
                    "final_answer": result.get("final_answer", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."),
                    "model_used": "langgraph_fallback",
                    "context": result.get("context", ""),
                    "datasources": result.get("datasources", []),
                    "success": True,
                    "error": None
                }
        
        # ==========================================
        # ğŸ¤– 3ë‹¨ê³„: ë³µì¡í•œ ì§ˆë¬¸ â†’ LangGraph ì—ì´ì „íŠ¸ ì‚¬ìš©
        # ==========================================
        else:
            print("\n" + "="*60)
            print("ë¼ìš°íŒ… ê²°ì •: LangGraph ì—ì´ì „íŠ¸ ì‚¬ìš© (ë³µì¡í•œ ì§ˆë¬¸)")
            print("="*60)
            
            # ì…ë ¥ ë°ì´í„° êµ¬ì„±
            inputs = {
                "question": question,
                "student_profile": student_profile,
                "recent_dialogues": recent_dialogues
            }
            
            # í†µí•© ì—ì´ì „íŠ¸ ì‹¤í–‰
            result = integrated_agent.invoke(
                inputs,
                config={
                    "recursion_limit": 25,  # ì¬ê·€ ì œí•œ
                    "timeout": 120          # 2ë¶„ íƒ€ì„ì•„ì›ƒ
                }
            )
            
            if not verbose:
                sys.stdout = old_stdout
            
            # ê²°ê³¼ ë°˜í™˜
            return {
                "question": question,
                "final_answer": result.get("final_answer", "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."),
                "model_used": "langgraph",
                "context": result.get("context", ""),
                "datasources": result.get("datasources", []),
                "success": True,
                "error": None
            }
        
    except Exception as e:
        if not verbose:
            sys.stdout = old_stdout
        
        error_msg = str(e)
        print(f"ì˜¤ë¥˜ ë°œìƒ: {error_msg[:200]}")
        
        return {
            "question": question,
            "final_answer": "ì˜¤ë¥˜ë¡œ ì¸í•´ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
            "model_used": "error",
            "context": "",
            "datasources": [],
            "success": False,
            "error": error_msg
        }


# ======================================
# ğŸ§ª API í…ŒìŠ¤íŠ¸ (ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í•  ë•Œ)
# ======================================
if __name__ == "__main__":
    print("=" * 80)
    print("CSmart API í…ŒìŠ¤íŠ¸ (ë¼ìš°íŒ… ê¸°ëŠ¥ í¬í•¨)")
    print("=" * 80)
    
    # í…ŒìŠ¤íŠ¸ 1: ê°„ë‹¨í•œ ì§ˆë¬¸ (íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš©)
    print("\n[í…ŒìŠ¤íŠ¸ 1] ê°„ë‹¨í•œ ì§ˆë¬¸ - íŒŒì¸íŠœë‹ ëª¨ë¸")
    print("-" * 80)
    result1 = get_answer(
        question="ìˆ˜í•™ ê³µë¶€ëŠ” ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?",
        verbose=True
    )
    
    print("\n" + "=" * 80)
    print("í…ŒìŠ¤íŠ¸ 1 ê²°ê³¼:")
    print("=" * 80)
    print(f"ì§ˆë¬¸: {result1['question']}")
    print(f"ì‚¬ìš©ëœ ëª¨ë¸: {result1['model_used']}")
    print(f"ì„±ê³µ ì—¬ë¶€: {result1['success']}")
    print(f"\në‹µë³€:\n{result1['final_answer']}")
    
    # í…ŒìŠ¤íŠ¸ 2: ë³µì¡í•œ ì§ˆë¬¸ (LangGraph ì‚¬ìš©)
    print("\n\n[í…ŒìŠ¤íŠ¸ 2] ë³µì¡í•œ ì§ˆë¬¸ - LangGraph ì—ì´ì „íŠ¸")
    print("-" * 80)
    result2 = get_answer(
        question="ì¤‘ì•™ëŒ€í•™êµ ì´ê³¼ í¸ì…ì€ ì–´ë–¤ ê³¼ëª©ì„ ì¤€ë¹„í•´ì•¼ í•˜ë‚˜ìš”?",
        student_profile={"target_university": "ì¤‘ì•™ëŒ€í•™êµ", "track": "ì´ê³¼"},
        recent_dialogues=[
            {"role": "student", "message": "í¸ì… ì¤€ë¹„ë¥¼ ì‹œì‘í•˜ë ¤ê³  í•©ë‹ˆë‹¤."},
            {"role": "teacher", "message": "ì¢‹ìŠµë‹ˆë‹¤. ì–´ëŠ ëŒ€í•™ì„ ëª©í‘œë¡œ í•˜ì‹œë‚˜ìš”?"},
            {"role": "student", "message": "ì¤‘ì•™ëŒ€í•™êµ ì´ê³¼ í¸ì…ì€ ì–´ë–¤ ê³¼ëª©ì„ ì¤€ë¹„í•´ì•¼ í•˜ë‚˜ìš”?"}
        ],
        verbose=True
    )
    
    print("\n" + "=" * 80)
    print("í…ŒìŠ¤íŠ¸ 2 ê²°ê³¼:")
    print("=" * 80)
    print(f"ì§ˆë¬¸: {result2['question']}")
    print(f"ì‚¬ìš©ëœ ëª¨ë¸: {result2['model_used']}")
    print(f"ì„±ê³µ ì—¬ë¶€: {result2['success']}")
    print(f"ì‚¬ìš©ëœ ë°ì´í„° ì†ŒìŠ¤: {result2['datasources']}")
    print(f"\në‹µë³€:\n{result2['final_answer']}")
    
    # í…ŒìŠ¤íŠ¸ 3: ìˆ˜ë™ ì„ íƒ ëª¨ë“œ í…ŒìŠ¤íŠ¸
    print("\n\n[í…ŒìŠ¤íŠ¸ 3] ìˆ˜ë™ ì„ íƒ ëª¨ë“œ - ê°„ë‹¨í•œ ì§ˆë¬¸ì„ LangGraphë¡œ")
    print("-" * 80)
    result3 = get_answer(
        question="ì˜ì–´ ë‹¨ì–´ ì•”ê¸°ëŠ” ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?",
        force_mode="complex",  # ìˆ˜ë™ìœ¼ë¡œ LangGraph ì„ íƒ
        verbose=True
    )
    
    print("\n" + "=" * 80)
    print("í…ŒìŠ¤íŠ¸ 3 ê²°ê³¼:")
    print("=" * 80)
    print(f"ì§ˆë¬¸: {result3['question']}")
    print(f"ì‚¬ìš©ëœ ëª¨ë¸: {result3['model_used']}")
    print(f"ì„±ê³µ ì—¬ë¶€: {result3['success']}")
    
    print("\n" + "=" * 80)
    print("ëª¨ë“  API í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("=" * 80)
    print("\n[ìš”ì•½]")
    print(f"í…ŒìŠ¤íŠ¸ 1 (ê°„ë‹¨í•œ ì§ˆë¬¸): {result1['model_used']} ì‚¬ìš©")
    print(f"í…ŒìŠ¤íŠ¸ 2 (ë³µì¡í•œ ì§ˆë¬¸): {result2['model_used']} ì‚¬ìš©")
    print(f"í…ŒìŠ¤íŠ¸ 3 (ìˆ˜ë™ ì„ íƒ ëª¨ë“œ): {result3['model_used']} ì‚¬ìš©")
    print("=" * 80)
