# Cell 19
from langgraph.graph import StateGraph, START, END
from langchain_core.prompts import ChatPromptTemplate
from typing import Literal, List, Dict
from pprint import pprint
try:
    from IPython.display import Image, display
except ImportError:
    # IPythonì´ ì—†ìœ¼ë©´ ë¬´ì‹œ (ë…¸íŠ¸ë¶ í™˜ê²½ì´ ì•„ë‹Œ ê²½ìš°)
    Image = None
    display = None
import datetime
import re
from step2_states import QAState
from step3_db_and_search import guideline_search
from step4_llm import llm


# ======================================
# 1âƒ£ GuidelineRagState ì •ì˜
# ======================================
class GuidelineRagState(QAState):
    """
    ì™„ì„±í¸ì… ì „ìš© Guideline ê²€ìƒ‰ìš© ìƒíƒœ êµ¬ì¡°
    QAStateë¥¼ í™•ì¥í•˜ì—¬ ì¶”ê°€ í•„ë“œ í¬í•¨
    """
    rewritten_query: str              # ì¬ì‘ì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬
    related_info: List[Dict]          # ì¶”ì¶œëœ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    node_answer: str                  # ìµœì¢… ë…¸ë“œ ë‹µë³€
    num_generations: int              # ë£¨í”„ ë°˜ë³µ íšŸìˆ˜
    sources: List[str]                # ìµœì¢… ì¶œì²˜ ë¦¬ìŠ¤íŠ¸


# ======================================
# 2âƒ£ ê³µí†µ ë¡œê·¸ í•¨ìˆ˜
# ======================================
def log(message: str, state: Dict = None):
    """ì‹œê°„ + ë‹¨ê³„ + ìƒíƒœ í‚¤ í‘œì‹œìš© ê³µìš© ë¡œê·¸ í•¨ìˆ˜"""
    print(f"\n[{datetime.datetime.now().strftime('%H:%M:%S')}] {message}")
    if state:
        keys = ', '.join(list(state.keys()))
        print(f"  -> í˜„ì¬ state keys: [{keys}]")


# ======================================
# 3âƒ£ Guideline ë¬¸ì„œ ê²€ìƒ‰ ë‹¨ê³„
# ======================================
def retrieve_guideline_docs(state: GuidelineRagState) -> GuidelineRagState:
    """
    GuidelineDBì˜ [question] ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ Embedding ê²€ìƒ‰ ìˆ˜í–‰
    """
    log("==== [1ë‹¨ê³„] retrieve_guideline_docs (questionì„ ê¸°ì¤€ìœ¼ë¡œ GuidelineDBì—ì„œ ê²€ìƒ‰ ìˆ˜í–‰) ì‹œì‘ ====", state)

    query = state.get("rewritten_query", state["question"])
    print(f"ğŸ“¢ ê²€ìƒ‰ ì¿¼ë¦¬ ì…ë ¥ê°’: {query}")
    print(" ê²€ìƒ‰ ê¸°ì¤€ í•„ë“œ: GuidelineDBì˜ [question] ì»¬ëŸ¼ (ì„ë² ë”© ë§¤ì¹­)")

    docs = guideline_search.invoke(query)
    print(f"ğŸ“„ ê²€ìƒ‰ ê²°ê³¼ ë¬¸ì„œ ìˆ˜: {len(docs)}")

    if len(docs) > 0:
        print("ğŸ§¾ Top 1 ê²€ìƒ‰ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
        preview_q = docs[0].page_content.split("\n")[0][:100]
        src_detail = docs[0].metadata.get("source_detail", "ì¶œì²˜ ë¯¸ê¸°ì¬")
        print(f"   â–¶ Q: {preview_q}")
        print(f"   â–¶ ì¶œì²˜: {src_detail}")
    else:
        print("â— ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì¿¼ë¦¬ë¥¼ ì¬ì‘ì„±í•˜ê±°ë‚˜ DBë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    # ì¶œì²˜ ëª©ë¡ì„ sources í•„ë“œì— ì €ì¥
    sources = [
        f"{doc.metadata.get('source_name', 'GuidelineDB')} ({doc.metadata.get('source_detail', 'ì¶œì²˜ ë¯¸ê¸°ì¬')})"
        for doc in docs
    ]

    return {"search_results": docs, "sources": sources}


# ======================================
# 4âƒ£ ë¬¸ì„œ ì •ë³´ ì¶”ì¶œ ë° í‰ê°€ ë‹¨ê³„
# ======================================
def extract_guideline_info(state: GuidelineRagState) -> GuidelineRagState:
    log("==== [2ë‹¨ê³„] extract_guideline_info (ë¬¸ì„œì—ì„œ ê´€ë ¨ í•µì‹¬ì •ë³´ ì¶”ì¶œ ë° í‰ê°€) ì‹œì‘ ====", state)
    extracted_list = []

    try:
        for i, doc in enumerate(state["search_results"]):
            print(f"\nğŸ§¾ {i+1}ë²ˆì§¸ ë¬¸ì„œ ë¶„ì„ ì¤‘...")
            src_detail = doc.metadata.get("source_detail", "ì¶œì²˜ ë¯¸ê¸°ì¬")
            print(f"   â–¶ ì¶œì²˜: {src_detail}")

            doc_q = doc.page_content
            doc_a = doc.metadata.get("answer", "")
            print(f"   Q: {doc_q[:100]}")
            print(f"   A: {doc_a[:100]}")

            prompt = ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ ëŒ€í•™ í¸ì… ëª¨ì§‘ìš”ê°• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
                ì•„ë˜ Q/A ë¬¸ì„œì—ì„œ í•™ìƒ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì£¼ìš” ì‚¬ì‹¤ì„ 3~5ê°œ ì •ë„ ì¶”ì¶œí•˜ì„¸ìš”. 
                ê° í•­ëª©ì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ì„ ë”°ë¦…ë‹ˆë‹¤:

                1. [ì¶”ì¶œëœ ì •ë³´ ìš”ì•½]
                - ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ê´€ë ¨ì„±: 0~1 ì‚¬ì´ ìˆ«ì
                - ì¶©ì‹¤ì„± ì ìˆ˜: 0~1 ì‚¬ì´ ìˆ«ì
                """),
                ("human", "ì§ˆë¬¸: {question}\n\n[ë¬¸ì„œ]\nQ: {q}\nA: {a}")
            ])
            formatted = prompt.format(question=state["question"], q=doc_q, a=doc_a)
            result = llm.invoke(formatted)

            if not result or not result.content.strip():
                print(" LLM ê²°ê³¼ ì—†ìŒ â†’ ë¬¸ì„œ ìŠ¤í‚µ")
                continue

            # --- ì ìˆ˜ ì¶”ì¶œ ---
            text = result.content.strip()
            relevance_scores = [float(x) for x in re.findall(r"ê´€ë ¨ì„±\s*ì ìˆ˜\s*[:ï¼š]?\s*([0-9]*\.?[0-9]+)", text)]
            faithfulness_scores = [float(x) for x in re.findall(r"ì¶©ì‹¤ì„±\s*ì ìˆ˜\s*[:ï¼š]?\s*([0-9]*\.?[0-9]+)", text)]

            avg_rel = sum(relevance_scores)/len(relevance_scores) if relevance_scores else 0
            avg_fai = sum(faithfulness_scores)/len(faithfulness_scores) if faithfulness_scores else 0

            print(f"   ì§ˆë¬¸ê³¼ ê´€ë ¨ì„±: {avg_rel:.2f}, ì¶©ì‹¤ì„± ì ìˆ˜: {avg_fai:.2f}")

            # --- ì ìˆ˜ ê¸°ì¤€ í•„í„°ë§ ---
            if avg_rel < 0.7 or avg_fai < 0.7:
                print("ì ìˆ˜ê°€ ë‚®ì•„ ì œì™¸ë¨ (ê¸°ì¤€: 0.7 ì´ìƒ)")
                continue

            extracted_list.append({
                "content": text,
                "source": src_detail,
                "avg_relevance": avg_rel,
                "avg_faithfulness": avg_fai
            })

        if len(extracted_list) == 0:
            print("â— ê´€ë ¨ ì •ë³´ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì ìˆ˜ ê¸°ì¤€ ë¯¸ë‹¬ì…ë‹ˆë‹¤.")

        log(" ì •ë³´ ì¶”ì¶œ ë° í•„í„°ë§ ì™„ë£Œ", {"ì¶”ì¶œëœ ì •ë³´ ê°œìˆ˜": len(extracted_list)})
        return {
            "related_info": extracted_list,
            "num_generations": state.get("num_generations", 0) + 1
        }

    except Exception as e:
        print(f" [ì˜¤ë¥˜] extract_guideline_info ì‹¤íŒ¨: {e}")
        return {"related_info": [], "num_generations": 0}


# ======================================
# 5âƒ£ ê²€ìƒ‰ ì¿¼ë¦¬ ì¬ì‘ì„± ë‹¨ê³„
# ======================================
def rewrite_guideline_query(state: GuidelineRagState) -> GuidelineRagState:
    """
    ì •ë³´ ë¶€ì¡± ì‹œ, LLMì„ í†µí•´ ê²€ìƒ‰ ì¿¼ë¦¬ ì¬ì‘ì„± ìˆ˜í–‰
    """
    log("==== [3ë‹¨ê³„] rewrite_guideline_query (ê²€ìƒ‰ ì¿¼ë¦¬ ì¬ì‘ì„±) ì‹œì‘ ====", state)
    info_text = "\n".join([i["content"] for i in state["related_info"]])

    rewrite_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ëŒ€í•™ í¸ì… ì „ë¬¸ ìƒë‹´ê°€ì…ë‹ˆë‹¤.
        ì•„ë˜ ì§ˆë¬¸ê³¼ ì¶”ì¶œëœ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë” êµ¬ì²´ì ì´ê³  ì •í™•í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë‹¤ì‹œ ì‘ì„±í•˜ì„¸ìš”.
        - í•µì‹¬ í‚¤ì›Œë“œ: í•™êµëª…, í•™ê³¼ëª…, ì§€ì›ìœ í˜•(ì¼ë°˜/í•™ì‚¬), ê³¼ëª©, ì¼ì •
        - í•œ ì¤„ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±
        """),
        ("human", "ì§ˆë¬¸: {question}\n\nì¶”ì¶œëœ ì •ë³´:\n{info}")
    ])

    rewritten = llm.invoke(rewrite_prompt.format(question=state["question"], info=info_text))
    new_query = rewritten.content.strip()

    print(f"ğŸ’¡ ì¬ì‘ì„±ëœ ì¿¼ë¦¬: {new_query}")
    return {"rewritten_query": new_query}


# ======================================
# 6âƒ£ ìµœì¢… ë‹µë³€ ìƒì„± ë‹¨ê³„
# ======================================
def generate_guideline_answer(state: GuidelineRagState) -> GuidelineRagState:
    """
    ëª¨ë“  ì¶”ì¶œ ì •ë³´ë¥¼ ì¢…í•©í•´ í•™ìƒ ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ë‹µë³€ ìƒì„±
    """
    log("==== [4ë‹¨ê³„] generate_guideline_answer (ìµœì¢… ë‹µë³€ ìƒì„±) ì‹œì‘ ====", state)

    # ì •ë³´ ë³‘í•© ë° ì¶œì²˜ í‘œì‹œ
    info_text = "\n".join([f"- {i['content']} (ì¶œì²˜: {i['source']})" for i in state["related_info"]])
    source_summary = "\n".join([f"- {s}" for s in state.get("sources", [])])

    answer_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ëŒ€í•™ í¸ì… ëª¨ì§‘ìš”ê°• ì „ë¬¸ ìƒë‹´ê°€ì…ë‹ˆë‹¤.
        í•™ìƒì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”.
        ë‹µë³€ì€ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ë©°, ê° ì •ë³´ì˜ ì¶œì²˜ë¥¼ ëª…í™•íˆ í‘œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.
        ì¶œë ¥ êµ¬ì¡°:
        1. í•µì‹¬ ìš”ì•½
        2. ì„¸ë¶€ ë‚´ìš©
        3. ì°¸ê³  ì¶œì²˜
        """),
        ("human", "ì§ˆë¬¸: {question}\n\nê´€ë ¨ ì •ë³´:\n{info}\n\nì°¸ê³  ì¶œì²˜:\n{src}")
    ])

    answer = llm.invoke(answer_prompt.format(
        question=state["question"],
        info=info_text,
        src=source_summary
    ))

    print("ğŸ—’ ìƒì„±ëœ ë‹µë³€ ë¯¸ë¦¬ë³´ê¸°:\n", answer.content[:300], "...")
    log(" ìµœì¢… ë‹µë³€ ìƒì„± ì™„ë£Œ")
    return {"node_answer": answer.content, "sources": state.get("sources", [])}


# ======================================
# 7âƒ£ íŒë‹¨ ë‹¨ê³„
# ======================================
def should_continue_guideline(state: GuidelineRagState) -> Literal["ê³„ì†", "ì¢…ë£Œ"]:
    """
    ì •ë³´ ì¶©ë¶„ ì—¬ë¶€ì— ë”°ë¼ ê·¸ë˜í”„ ì§„í–‰ ë°©í–¥ ê²°ì •
    """
    log("==== [íŒë‹¨ë‹¨ê³„] should_continue_guideline (ì •ë³´ ì¶©ì¡± ì—¬ë¶€ íŒë‹¨) ====", state)
    print("í˜„ì¬ related_info ê°œìˆ˜:", len(state.get("related_info", [])))
    print("í˜„ì¬ ë°˜ë³µ íšŸìˆ˜:", state.get("num_generations", 0))

    gen = state.get("num_generations", 0)
    info_count = len(state.get("related_info", []))

    if gen >= 2:
        print("ğŸ” ë°˜ë³µ íšŸìˆ˜ ì´ˆê³¼ â†’ ì¢…ë£Œ")
        return "ì¢…ë£Œ"

    if info_count > 0:
        print(f"ğŸ“ˆ ì¶©ë¶„í•œ ì •ë³´ í™•ë³´ ({info_count}ê°œ) â†’ ì¢…ë£Œ")
        return "ì¢…ë£Œ"

    print("ğŸ”„ ì •ë³´ ë¶€ì¡± â†’ ì¿¼ë¦¬ ì¬ì‘ì„± í›„ ì¬ê²€ìƒ‰")
    return "ê³„ì†"


# ======================================
# 8âƒ£ ê·¸ë˜í”„ êµ¬ì„± ë° ì»´íŒŒì¼
# ======================================
log(" [ì´ˆê¸°í™”] LangGraph Guideline Search Workflow êµ¬ì„± ì‹œì‘")

guideline_graph = StateGraph(GuidelineRagState)

guideline_graph.add_node("retrieve", retrieve_guideline_docs)
guideline_graph.add_node("extract", extract_guideline_info)
guideline_graph.add_node("rewrite", rewrite_guideline_query)
guideline_graph.add_node("answer", generate_guideline_answer)

guideline_graph.add_edge(START, "retrieve")
guideline_graph.add_edge("retrieve", "extract")

guideline_graph.add_conditional_edges(
    "extract",
    should_continue_guideline,
    {"ê³„ì†": "rewrite", "ì¢…ë£Œ": "answer"}
)

guideline_graph.add_edge("rewrite", "retrieve")
guideline_graph.add_edge("answer", END)

guideline_agent = guideline_graph.compile()
log(" [ì™„ë£Œ] Guideline Agent ì»´íŒŒì¼ ì™„ë£Œ")


# ======================================
# ğŸ¯ ì™„ì„±í¸ì… Guideline Graph (í‘œì¤€ ë…¸ë“œëª… ë²„ì „)
# ======================================

# ê·¸ë˜í”„ ìƒì„±
workflow = StateGraph(GuidelineRagState)

# ë…¸ë“œ ì¶”ê°€ (í‘œì¤€í™”ëœ ì´ë¦„ ì‚¬ìš©)
workflow.add_node("retrieve_documents", retrieve_guideline_docs)       # GuidelineDB ê²€ìƒ‰
workflow.add_node("extract_and_evaluate", extract_guideline_info)      # ì •ë³´ ì¶”ì¶œ ë° ì ìˆ˜ í‰ê°€
workflow.add_node("rewrite_query", rewrite_guideline_query)            # ê²€ìƒ‰ ì¿¼ë¦¬ ì¬ì‘ì„±
workflow.add_node("generate_answer", generate_guideline_answer)        # ìµœì¢… ë‹µë³€ ìƒì„±

# ì—£ì§€ ì—°ê²°
workflow.add_edge(START, "retrieve_documents")
workflow.add_edge("retrieve_documents", "extract_and_evaluate")

# ì¡°ê±´ë¶€ ì—£ì§€ ì—°ê²°
workflow.add_conditional_edges(
    "extract_and_evaluate",
    should_continue_guideline,  # íŒë‹¨ ë¡œì§
    {
        "ê³„ì†": "rewrite_query",
        "ì¢…ë£Œ": "generate_answer"
    }
)

# ë£¨í”„: ì¿¼ë¦¬ ì¬ì‘ì„± í›„ ë‹¤ì‹œ ê²€ìƒ‰
workflow.add_edge("rewrite_query", "retrieve_documents")

# ìµœì¢… ë‹µë³€ í›„ ì¢…ë£Œ
workflow.add_edge("generate_answer", END)

# ê·¸ë˜í”„ ì»´íŒŒì¼
guideline_agent = workflow.compile()

# ======================================
# ğŸ§­ ê·¸ë˜í”„ ì‹œê°í™”
# ======================================
if display is not None and Image is not None:
    display(Image(guideline_agent.get_graph().draw_mermaid_png()))
    print("\n [ì™„ë£Œ] Guideline Agent ê·¸ë˜í”„ êµ¬ì¡° ì‹œê°í™” ì™„ë£Œ ")
else:
    print("\n [ì™„ë£Œ] Guideline Agent ì»´íŒŒì¼ ì™„ë£Œ (ê·¸ë˜í”„ ì‹œê°í™”ëŠ” Jupyter í™˜ê²½ì—ì„œë§Œ ê°€ëŠ¥) ")

# ======================================
#  íŠ¸ë¦¬ êµ¬ì¡° í…ìŠ¤íŠ¸ ë¡œê·¸
# ======================================
print("\nGuideline Workflow êµ¬ì¡°:")
print("""
START â†’ retrieve_documents â†’ extract_and_evaluate
 â”œâ”€(ê³„ì†)â†’ rewrite_query â†’ retrieve_documents
 â””â”€(ì¢…ë£Œ)â†’ generate_answer â†’ END
""")
